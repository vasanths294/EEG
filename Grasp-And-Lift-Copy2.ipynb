{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############function to read data###########\n",
    "\n",
    "def prepare_data_train(fname):\n",
    "    \"\"\" read and prepare training data \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(fname)\n",
    "    # events file\n",
    "    events_fname = fname.replace('_data','_events')\n",
    "    # read event file\n",
    "    labels= pd.read_csv(events_fname)\n",
    "    clean=data.drop(['id' ], axis=1)#remove id\n",
    "    labels=labels.drop(['id' ], axis=1)#remove id\n",
    "    return  clean,labels\n",
    "\n",
    "def prepare_data_test(fname):\n",
    "    \"\"\" read and prepare test data \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(fname)\n",
    "    return data\n",
    "\n",
    "scaler= StandardScaler()\n",
    "def data_preprocess_train(X):\n",
    "    X_prep=scaler.fit_transform(X)\n",
    "    #do here your preprocessing\n",
    "    return X_prep\n",
    "def data_preprocess_test(X):\n",
    "    X_prep=scaler.transform(X)\n",
    "    #do here your preprocessing\n",
    "    return X_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##downsamplig naive like this is not correct, if you do not low pass filter.\n",
    "##this down sampling here it needed only to keep the script run below 10 minutes.\n",
    "## please do not downsample or use correct procedure to decimate data without alias\n",
    "subsample=100 # training subsample.if you want to downsample the training data\n",
    "#######columns name for labels#############\n",
    "cols = ['HandStart','FirstDigitTouch',\n",
    "        'BothStartLoadPhase','LiftOff',\n",
    "        'Replace','BothReleased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######number of subjects###############\n",
    "#subjects = range(1,13)\n",
    "subjects = range(1,2)\n",
    "ids_tot = []\n",
    "pred_tot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subject 1, class HandStart\n",
      "0.960717259062835\n",
      "Train subject 1, class FirstDigitTouch\n",
      "0.9596918774473701\n",
      "Train subject 1, class BothStartLoadPhase\n",
      "0.9591514239714499\n",
      "Train subject 1, class LiftOff\n",
      "0.9603447474281391\n",
      "Train subject 1, class Replace\n",
      "0.9635179487924427\n",
      "Train subject 1, class BothReleased\n",
      "0.9608258114505278\n"
     ]
    }
   ],
   "source": [
    "###loop on subjects and 8 series for train data + 2 series for test data\n",
    "for subject in subjects:\n",
    "    y_raw= []\n",
    "    raw = []\n",
    "    ################ READ DATA ################################################\n",
    "    fnames =  glob('./train/subj%d_series*_data.csv' % (subject))\n",
    "    for fname in fnames:\n",
    "      data,labels=prepare_data_train(fname)\n",
    "      raw.append(data)\n",
    "      y_raw.append(labels)\n",
    "\n",
    "    X = pd.concat(raw)\n",
    "    y = pd.concat(y_raw)\n",
    "    #transform in numpy array\n",
    "    #transform train data in numpy array\n",
    "    X_train =np.asarray(X.astype(float))\n",
    "    y = np.asarray(y.astype(float))\n",
    "\n",
    "\n",
    "    ################ Read test data #####################################\n",
    "    #\n",
    "    fnames =  glob('./train/subj%d_series*_data.csv' % (subject))\n",
    "    test = []\n",
    "    idx=[]\n",
    "    for fname in fnames:\n",
    "      data=prepare_data_test(fname)\n",
    "      test.append(data)\n",
    "      idx.append(np.array(data['id']))\n",
    "    X_test= pd.concat(test)\n",
    "    ids=np.concatenate(idx)\n",
    "    ids_tot.append(ids)\n",
    "    X_test=X_test.drop(['id' ], axis=1)#remove id\n",
    "    #transform test data in numpy array\n",
    "    X_test =np.asarray(X_test.astype(float))\n",
    "\n",
    "\n",
    "    ################ Train classifiers ########################################\n",
    "    lr = LogisticRegression()\n",
    "    # from sklearn.svm import SVC\n",
    "    # lr = SVC()\n",
    "    pred = np.empty((X_test.shape[0],6))\n",
    "    X_train=data_preprocess_train(X_train)\n",
    "    X_test=data_preprocess_test(X_test)\n",
    "    for i in range(6):\n",
    "        y_train= y[:,i]\n",
    "        print('Train subject %d, class %s' % (subject, cols[i]))\n",
    "        lr.fit(X_train[::subsample,:],y_train[::subsample])\n",
    "        pred[:,i] = lr.predict_proba(X_test)[:,1]\n",
    "        y_train_pred = lr.predict_proba(X_train)[:,1]\n",
    "        print(1-mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "    pred_tot.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119496, 6)\n",
      "(119496, 32)\n"
     ]
    }
   ],
   "source": [
    "print(pred_tot[0].shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission file\n",
    "submission_file = 'grasp-sub-simple.csv'\n",
    "# create pandas object for sbmission\n",
    "submission = pd.DataFrame(index=np.concatenate(ids_tot),\n",
    "                          columns=cols,\n",
    "                          data=np.concatenate(pred_tot))\n",
    "\n",
    "# write file\n",
    "submission.to_csv(submission_file,index_label='id',float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
